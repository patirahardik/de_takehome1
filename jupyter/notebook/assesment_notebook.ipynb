{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding required packages\n",
    "import findspark\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unittest related packages\n",
    "from unittest import mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pyspark related package\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local System Configuration\n",
    "# Total Memory = 16GB\n",
    "# Total Cores = 10\n",
    "\n",
    "def get_spark_conf():\n",
    "    # Configure Spark settings\n",
    "    spark_conf = SparkConf()\n",
    "    spark_conf.set(\"spark.executor.instances\", \"4\") # 2 instance per noce\n",
    "\n",
    "    # Set the number of executor cores\n",
    "    spark_conf.set(\"spark.executor.cores\", \"2\")  # Use 2 cores per executor\n",
    "\n",
    "    # Set the executor memory\n",
    "    spark_conf.set(\"spark.executor.memory\", \"4g\")  # Use 4GB memory per executor\n",
    "\n",
    "    # Set the driver memory\n",
    "    spark_conf.set(\"spark.driver.memory\", \"2g\")    # Use 2GB memory for the driver\n",
    "    \n",
    "    return spark_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_value_counts(df):\n",
    "    missing_value_counts = df.select([(f.count(f.when(f.col(c).contains('None') |\n",
    "                                  f.col(c).contains('NULL') |\n",
    "                                  (f.col(c) == '') |\n",
    "                                  f.col(c).isNull() |\n",
    "                                  f.isnan(c), c)).alias(c + \"_missing\"))\n",
    "                       for c in df.columns])\n",
    "    return missing_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics for numerical columns\n",
    "def calculate_summary_stats(df, numerical_columns):\n",
    "    summary_stats = df.select(*numerical_columns).summary(\"mean\", \"stddev\", \"min\", \"max\")\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_categorical_column(df, col_name):\n",
    "    distinct_values = df.select(col_name).distinct().count()\n",
    "    top_values = df.groupBy(col_name).count().orderBy(f.col(\"count\").desc()).limit(5)\n",
    "    return distinct_values, top_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_column(df, threshold=0.3):\n",
    "    categorical_columns = []\n",
    "    for col_name in df.columns:\n",
    "        distinct_count = df.select(col_name).distinct().count()\n",
    "        if distinct_count < df.count() * threshold:  # Adjust threshold as needed\n",
    "            categorical_columns.append(col_name)\n",
    "\n",
    "    return categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Listing the columns based on its type\n",
    "# def get_col_type_dict(df):\n",
    "#     col_type_dict = {}\n",
    "\n",
    "#     for col_name, col_type in df.dtypes:\n",
    "#         if col_type in col_type_dict.keys():\n",
    "#             col_type_dict[col_type].append(col_name)\n",
    "#         else:\n",
    "#             col_type_dict[col_type] = [col_name]\n",
    "\n",
    "#     return col_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_profile(df):\n",
    "    \n",
    "    print(\"Starting Data Profiling\")\n",
    "    print(\"Schema of the dataset\")\n",
    "    \n",
    "    #getting the schema\n",
    "    df.printSchema()\n",
    "    \n",
    "#     ## Display the first few rows of the DataFrame\n",
    "#     display(df.limit(5))\n",
    "    \n",
    "#     # Getting counts\n",
    "#     total_count = df.count()\n",
    "#     print(f\"Total Records: {total_count}\")\n",
    "    \n",
    "    \n",
    "#     print(\"Getting missing value counts\")\n",
    "#     missing_value_counts = calculate_missing_value_counts(df)\n",
    "#     display(missing_value_counts)\n",
    "    \n",
    "#     print(\"Getting numerical status\")\n",
    "#     numerical_columns = [col_name for col_name, col_type in df.dtypes if col_type in [\"int\", \"double\", \"float\"]]\n",
    "#     summary_stats = calculate_summary_stats(df, numerical_columns)\n",
    "#     display(summary_stats)\n",
    "    \n",
    "#     print(\"Getting categorical columns\")\n",
    "#     categorical_columns = get_categorical_column(df, 0.01)\n",
    "#     for col_name in categorical_columns:\n",
    "#         distinct_values, top_values = profile_categorical_column(df, col_name)\n",
    "#         print(f\"Column: {col_name}\")\n",
    "#         print(f\"Distinct Values: {distinct_values}\")\n",
    "#         print(\"Top Values:\")\n",
    "#         top_values.show(100,False)\n",
    "        \n",
    "    print(\"Getting KPIs\")\n",
    "    # KPI 1: Number of job postings per category (Top 10)\n",
    "    category_counts = df.groupBy(\"Job Category\").count().orderBy(f.col(\"count\").desc()).limit(10)\n",
    "    category_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #setting spark conf before creating spark session\n",
    "    spark_conf = get_spark_conf()\n",
    "    \n",
    "    # Create a SparkSession with the configured settings\n",
    "    spark = SparkSession.builder.config(conf=spark_conf).appName(\"MySparkApp\").getOrCreate()\n",
    "    \n",
    "    # Listing all the spark conf\n",
    "    spark.sparkContext.getConf().getAll()\n",
    "    \n",
    "    # setting spark conf for analysis\n",
    "    spark.conf.set('spark.sql.repl.eagerEval.enabled',True)\n",
    "    \n",
    "    #reading dataset\n",
    "    df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # reducing the shuffle partition to 4 \n",
    "    # reason 1 data size is very less\n",
    "    # reason 2 to use all the availble cores\n",
    "    spark.conf.set('spark.sql.shuffle.partitions',4)\n",
    "    \n",
    "    # Creating data profile\n",
    "    data_profile(df)\n",
    "    \n",
    "    spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Profiling\n",
      "Schema of the dataset\n",
      "root\n",
      " |-- Job ID: integer (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: integer (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: double (nullable = true)\n",
      " |-- Salary Range To: double (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: string (nullable = true)\n",
      " |-- Post Until: string (nullable = true)\n",
      " |-- Posting Updated: string (nullable = true)\n",
      " |-- Process Date: string (nullable = true)\n",
      "\n",
      "Getting KPIs\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
