{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding required packages\n",
    "import findspark\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make pyspark importable as a regular library\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pyspark related package\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import col, when, avg, round, rank, isnan, current_timestamp, date_sub\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profile Function Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/lib/data_profiling_transform.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPI Transformations Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/lib/kpi_transform.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/utils/spark_session.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_conf():\n",
    "    # Configure Spark settings\n",
    "    spark_conf = SparkConf()\n",
    "    spark_conf.set(\"spark.executor.instances\", \"4\") # 4 instance per node\n",
    "\n",
    "    # Set the number of executor cores\n",
    "    spark_conf.set(\"spark.executor.cores\", \"2\")  # Use 2 cores per executor\n",
    "\n",
    "    # Set the executor memory\n",
    "    spark_conf.set(\"spark.executor.memory\", \"1g\")  # Use 1GB memory per executor\n",
    "\n",
    "    # Set the driver memory\n",
    "    spark_conf.set(\"spark.driver.memory\", \"2g\")    # Use 2GB memory for the driver\n",
    "    \n",
    "    return spark_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'nyc_assesment'\n",
    "    \n",
    "#setting spark conf before creating spark session\n",
    "spark_conf = get_spark_conf()\n",
    "\n",
    "# Create a SparkSession with the configured settings\n",
    "spark = get_spark_session(spark_conf, job_name)\n",
    "\n",
    "# Listing all the spark conf\n",
    "spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# setting spark conf for analysis\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataset\n",
    "# adding escape charater after data profiling the data \n",
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, inferSchema=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the shuffle partition to 4 \n",
    "# reason 1 data size is very less\n",
    "# reason 2 to use all the availble cores\n",
    "# reason 3 Since data size is very small and can be fitted in single partition but using 4 just to show data shuffling\n",
    "spark.conf.set('spark.sql.shuffle.partitions',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first few rows of the DataFrame\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting counts\n",
    "total_count = df.count()\n",
    "print(f\"Total Records: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "df = df.distinct()\n",
    "dist_tot_cnt = df.count()\n",
    "print(f\"Total Records after removing duplicates: {dist_tot_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting missing value counts\")\n",
    "missing_value_counts = calculate_missing_value_counts(df)\n",
    "display(missing_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting numerical status\")\n",
    "numerical_columns = [col_name for col_name, col_type in df.dtypes if col_type in [\"int\", \"double\", \"float\"]]\n",
    "summary_stats = calculate_summary_stats(df, numerical_columns)\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Getting categorical columns\") \n",
    "# Most of the time categorical column should be string type but for this analysis -\n",
    "# assuming that categorical columns can be any type.\n",
    "distinct_threshold=10\n",
    "for col_name in df.columns:\n",
    "    is_categorical_cond, distinct_values, top_values = profile_categorical_column(df, col_name)\n",
    "    if is_categorical_cond:\n",
    "        print(f\"Column: {col_name}\")\n",
    "        print(f\"Distinct Values: {distinct_values}\")\n",
    "        print(\"Top Values:\")\n",
    "        top_values.show(truncate=False)\n",
    "        pandas_df = top_values.toPandas()\n",
    "        # Plot the pie chart using Pandas\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        pandas_df.plot.pie(y=\"count\", labels=pandas_df[f\"{col_name}\"], autopct=\"%1.1f%%\", startangle=140)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.title(f\"{col_name} Distribution\")\n",
    "        plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ydata_profiling to create the data profile based on pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(pandas_df, title=\"Profiling Report New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937aa05b886546128b0787f1b2eeb909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0da0c3870244849b5ddd8a1d80b197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05af0e18e77749c4b368db564f3e4268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_kpi(df):        \n",
    "    # Salary range is linked with salary frequency. \n",
    "    # To get any metrics which depends on salary we need to have salary in same frequency.    \n",
    "    print(\"Preparing Data for KPI\")\n",
    "    kpi_pre_df = df.withColumn(\"Annual Salary From\",\n",
    "                       when(col(\"Salary Frequency\") == \"Hourly\", col(\"Salary Range From\") * 2080)  # Assuming 2080 work hours per year\n",
    "                       .when(col(\"Salary Frequency\") == \"Weekly\", col(\"Salary Range From\") * 52)\n",
    "                       .when(col(\"Salary Frequency\") == \"Daily\", col(\"Salary Range From\") * 260)  # Assuming 5 workdays per week\n",
    "                       .otherwise(col(\"Salary Range From\")))\n",
    "    \n",
    "    kpi_df = kpi_pre_df.withColumn(\"Annual Salary To\",\n",
    "                       when(col(\"Salary Frequency\") == \"Hourly\", col(\"Salary Range To\") * 2080)\n",
    "                       .when(col(\"Salary Frequency\") == \"Weekly\", col(\"Salary Range To\") * 52)\n",
    "                       .when(col(\"Salary Frequency\") == \"Daily\", col(\"Salary Range To\") * 260)\n",
    "                       .otherwise(col(\"Salary Range To\")))\n",
    "    \n",
    "    \n",
    "    print(\"Getting KPIs\")\n",
    "    print(\"Top 10 jobs posting per category\")\n",
    "    category_counts = get_top10_job_posting_per_cat(kpi_df)\n",
    "    category_counts.show(truncate=False)\n",
    "    \n",
    "    print(\"The salary distribution per job category\")\n",
    "    salary_distribution = get_sal_dist_per_cat(kpi_df)\n",
    "    salary_distribution.show(truncate=False)\n",
    "    \n",
    "    print(\"The job posting having the highest salary per agency\")\n",
    "    max_sal_per_agency_df = get_highes_sal_per_cat(kpi_df)\n",
    "    max_sal_per_agency_df.show(truncate=False)\n",
    "    \n",
    "    #KPI5: Whats the job positings average salary per agency for the last 2 years\n",
    "    print(\"The average salary per agency for the last 2 years\")\n",
    "    last_n_year = 2\n",
    "    avg_salary_df = get_avg_sal_per_agency_last_n_year(kpi_df,last_n_year)\n",
    "    avg_salary_df.show(truncate=False)\n",
    "\n",
    "    # KPI6 6: What are the highest paid skills in the US market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
