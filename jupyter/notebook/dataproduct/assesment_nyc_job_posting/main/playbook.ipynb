{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding required packages\n",
    "import findspark\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make pyspark importable as a regular library\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pyspark related package\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import col, when, avg, round, rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/utils/spark_session.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local System Configuration\n",
    "# Total Memory = 16GB\n",
    "# Total Cores = 10\n",
    "\n",
    "def get_spark_conf():\n",
    "    # Configure Spark settings\n",
    "    spark_conf = SparkConf()\n",
    "    spark_conf.set(\"spark.executor.instances\", \"4\") # 4 instance per node\n",
    "\n",
    "    # Set the number of executor cores\n",
    "    spark_conf.set(\"spark.executor.cores\", \"1\")  # Use 1 cores per executor\n",
    "\n",
    "    # Set the executor memory\n",
    "    spark_conf.set(\"spark.executor.memory\", \"1g\")  # Use 1GB memory per executor\n",
    "\n",
    "    # Set the driver memory\n",
    "    spark_conf.set(\"spark.driver.memory\", \"2g\")    # Use 2GB memory for the driver\n",
    "    \n",
    "    return spark_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting spark conf before creating spark session\n",
    "spark_conf = get_spark_conf()\n",
    "\n",
    "# Create a SparkSession with the configured settings\n",
    "spark = SparkSession.builder.config(conf=spark_conf).appName(\"MySparkApp\").getOrCreate()\n",
    "\n",
    "# Listing all the spark conf\n",
    "spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# setting spark conf for analysis\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled',True)\n",
    "\n",
    "#reading dataset\n",
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, inferSchema=True, escape='\"')\n",
    "\n",
    "# reducing the shuffle partition to 4 \n",
    "# reason 1 data size is very less\n",
    "# reason 2 to use all the availble cores\n",
    "spark.conf.set('spark.sql.shuffle.partitions',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Functions Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/lib/data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "#df = df.select('Preferred Skills', f.coalesce(f.regexp_replace(\"Preferred Skills\", \"[^\\w\\s]+\", \"\"),f.lit('')).alias('preferred_skill_non_punch'))\n",
    "Cleaned_df = text_preparation(df, 'Preferred Skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Preferred Skills_splitted`' given input columns: [Job Description, Process Date, Residency Requirement, Division/Work Unit, Agency, Preferred Skills_prepared, Recruitment Contact, # Of Positions, Title Code No, Salary Range To, Hours/Shift, Post Until, Full-Time/Part-Time indicator, Posting Type, Salary Range From, Posting Date, Business Title, Preferred Skills, Job Category, Additional Information, Minimum Qual Requirements, Level, Salary Frequency, Work Location, Work Location 1, Civil Service Title, To Apply, Posting Updated, Job ID];;\\n'Project [Preferred Skills#27, Preferred Skills_prepared#554, 'Preferred Skills_splitted]\\n+- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 5 more fields]\\n   +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 8 more fields]\\n      +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 8 more fields]\\n         +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 7 more fields]\\n            +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 6 more fields]\\n               +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 5 more fields]\\n                  +- Relation[Job ID#10,Agency#11,Posting Type#12,# Of Positions#13,Business Title#14,Civil Service Title#15,Title Code No#16,Level#17,Job Category#18,Full-Time/Part-Time indicator#19,Salary Range From#20,Salary Range To#21,Salary Frequency#22,Work Location#23,Division/Work Unit#24,Job Description#25,Minimum Qual Requirements#26,Preferred Skills#27,Additional Information#28,To Apply#29,Hours/Shift#30,Work Location 1#31,Recruitment Contact#32,Residency Requirement#33,... 4 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o330.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Preferred Skills_splitted`' given input columns: [Job Description, Process Date, Residency Requirement, Division/Work Unit, Agency, Preferred Skills_prepared, Recruitment Contact, # Of Positions, Title Code No, Salary Range To, Hours/Shift, Post Until, Full-Time/Part-Time indicator, Posting Type, Salary Range From, Posting Date, Business Title, Preferred Skills, Job Category, Additional Information, Minimum Qual Requirements, Level, Salary Frequency, Work Location, Work Location 1, Civil Service Title, To Apply, Posting Updated, Job ID];;\n'Project [Preferred Skills#27, Preferred Skills_prepared#554, 'Preferred Skills_splitted]\n+- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 5 more fields]\n   +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 8 more fields]\n      +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 8 more fields]\n         +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 7 more fields]\n            +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 6 more fields]\n               +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 5 more fields]\n                  +- Relation[Job ID#10,Agency#11,Posting Type#12,# Of Positions#13,Business Title#14,Civil Service Title#15,Title Code No#16,Level#17,Job Category#18,Full-Time/Part-Time indicator#19,Salary Range From#20,Salary Range To#21,Salary Frequency#22,Work Location#23,Division/Work Unit#24,Job Description#25,Minimum Qual Requirements#26,Preferred Skills#27,Additional Information#28,To Apply#29,Hours/Shift#30,Work Location 1#31,Recruitment Contact#32,Residency Requirement#33,... 4 more fields] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-391cff937f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCleaned_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preferred Skills'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Preferred Skills_prepared'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Preferred Skills_splitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Preferred Skills_splitted`' given input columns: [Job Description, Process Date, Residency Requirement, Division/Work Unit, Agency, Preferred Skills_prepared, Recruitment Contact, # Of Positions, Title Code No, Salary Range To, Hours/Shift, Post Until, Full-Time/Part-Time indicator, Posting Type, Salary Range From, Posting Date, Business Title, Preferred Skills, Job Category, Additional Information, Minimum Qual Requirements, Level, Salary Frequency, Work Location, Work Location 1, Civil Service Title, To Apply, Posting Updated, Job ID];;\\n'Project [Preferred Skills#27, Preferred Skills_prepared#554, 'Preferred Skills_splitted]\\n+- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 5 more fields]\\n   +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 8 more fields]\\n      +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 8 more fields]\\n         +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 7 more fields]\\n            +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 6 more fields]\\n               +- Project [Job ID#10, Agency#11, Posting Type#12, # Of Positions#13, Business Title#14, Civil Service Title#15, Title Code No#16, Level#17, Job Category#18, Full-Time/Part-Time indicator#19, Salary Range From#20, Salary Range To#21, Salary Frequency#22, Work Location#23, Division/Work Unit#24, Job Description#25, Minimum Qual Requirements#26, Preferred Skills#27, Additional Information#28, To Apply#29, Hours/Shift#30, Work Location 1#31, Recruitment Contact#32, Residency Requirement#33, ... 5 more fields]\\n                  +- Relation[Job ID#10,Agency#11,Posting Type#12,# Of Positions#13,Business Title#14,Civil Service Title#15,Title Code No#16,Level#17,Job Category#18,Full-Time/Part-Time indicator#19,Salary Range From#20,Salary Range To#21,Salary Frequency#22,Work Location#23,Division/Work Unit#24,Job Description#25,Minimum Qual Requirements#26,Preferred Skills#27,Additional Information#28,To Apply#29,Hours/Shift#30,Work Location 1#31,Recruitment Contact#32,Residency Requirement#33,... 4 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "Cleaned_df.select('Preferred Skills','Preferred Skills_prepared','Preferred Skills_splitted').limit(5).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
