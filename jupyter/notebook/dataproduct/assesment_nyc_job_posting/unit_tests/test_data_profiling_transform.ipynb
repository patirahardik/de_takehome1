{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (7.4.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest) (2.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.1.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipytest in /usr/local/lib/python3.7/dist-packages (0.13.3)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.13.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (23.1)\n",
      "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.4.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.1.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.6.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython->ipytest) (40.8.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.16.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (3.0.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.1.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.1.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.6.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.1.9)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipytest) (1.14.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding required packages\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make pyspark importable as a regular library\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pyspark related package\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profile Function Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/lib/data_profiling_transform.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/utils/spark_session.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_conf():\n",
    "    # Configure Spark settings\n",
    "    spark_conf = SparkConf()\n",
    "    spark_conf.set(\"spark.executor.instances\", \"4\") # 4 instance per node\n",
    "\n",
    "    # Set the number of executor cores\n",
    "    spark_conf.set(\"spark.executor.cores\", \"2\")  # Use 2 cores per executor\n",
    "\n",
    "    # Set the executor memory\n",
    "    spark_conf.set(\"spark.executor.memory\", \"1g\")  # Use 1GB memory per executor\n",
    "\n",
    "    # Set the driver memory\n",
    "    spark_conf.set(\"spark.driver.memory\", \"2g\")    # Use 2GB memory for the driver\n",
    "    \n",
    "    return spark_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'UnitTest'\n",
    "\n",
    "#setting spark conf before creating spark session\n",
    "spark_conf = get_spark_conf()\n",
    "    \n",
    "# Create a SparkSession with the configured settings\n",
    "spark = get_spark_session(spark_conf, job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 19.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_calculate_missing_value_counts():\n",
    "    # Create a Spark DataFrame for testing\n",
    "    data = [\n",
    "        (87990, \"Internal\", None),\n",
    "        (87991, None, \"1\"),\n",
    "        (87992, \"External\", \"NULL\"),\n",
    "        (87993, \"Internal\", \"M3\"),\n",
    "        (87994, \"External\", \"M2\")\n",
    "    ]\n",
    "    columns = [\"Job ID\", \"Posting Type\", \"Level\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Call the function to be tested\n",
    "    result_df = calculate_missing_value_counts(df)\n",
    "\n",
    "    # Define the expected result DataFrame\n",
    "    expected_data = [\n",
    "        (0, 1, 2)\n",
    "    ]\n",
    "    expected_columns = [\"Job ID_missing\", \"Posting Type_missing\", \"Level_missing\"]\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_columns)\n",
    "\n",
    "    # Compare the actual and expected DataFrames\n",
    "    assert result_df.collect() == expected_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 3.71s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Test the calculate_summary_stats function\n",
    "def test_calculate_summary_stats():\n",
    "    \n",
    "    data = [\n",
    "        (1, 5, 10),\n",
    "        (2, 10, 20),\n",
    "        (3, 15, 30),\n",
    "    ]\n",
    "    columns = [\"# Of Positions\", \"Salary Range From\", \"Salary Range To\"]\n",
    "    \n",
    "    \n",
    "    numerical_columns = [\"# Of Positions\", \"Salary Range From\", \"Salary Range To\"]\n",
    "    \n",
    "    test_data = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    result = calculate_summary_stats(test_data, numerical_columns)\n",
    "    \n",
    "    # Ensure that the resulting DataFrame has the expected columns\n",
    "    expected_columns = [\"summary\", \"# Of Positions\", \"Salary Range From\", \"Salary Range To\"]\n",
    "    assert result.columns == expected_columns\n",
    "    \n",
    "    # Ensure that the \"mean\" value for \"# Of Positions\" matches the expected value\n",
    "    assert result.filter(result[\"summary\"] == \"mean\").select(\"# Of Positions\").collect()[0][\"# Of Positions\"] == '2.0'\n",
    "    \n",
    "    # Ensure that the \"max\" value for \"Salary Range From\" matches the expected value\n",
    "    assert result.filter(result[\"summary\"] == \"max\").select(\"Salary Range From\").collect()[0][\"Salary Range From\"] == '15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 12.34s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Test the is_categorical function\n",
    "def test_is_categorical():\n",
    "    \n",
    "    data = [\n",
    "        (\"Internal\", 2),\n",
    "        (\"External\", 20),\n",
    "        (\"Internal\", 3),\n",
    "        (\"External\", 40),\n",
    "        (\"Internal\", 5),\n",
    "        (\"External\", 60),\n",
    "    ]\n",
    "    columns = [\"Posting Type\", \"# Of Positions\"]\n",
    "    test_data = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    \n",
    "    # Test a categorical column with distinct count above threshold\n",
    "    is_cat, distinct_count = is_categorical(test_data, \"Posting Type\",2)\n",
    "    assert is_cat is True\n",
    "    assert distinct_count == 2\n",
    "    \n",
    "    # Test a categorical column with distinct count below threshold\n",
    "    is_cat, distinct_count = is_categorical(test_data, \"# Of Positions\",2)\n",
    "    assert is_cat is False\n",
    "    assert distinct_count == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 9.72s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Test the is_categorical function\n",
    "def test_is_categorical():\n",
    "    \n",
    "    data = [\n",
    "        (\"Internal\", 2),\n",
    "        (\"External\", 20),\n",
    "        (\"Internal\", 3),\n",
    "        (\"External\", 40),\n",
    "        (\"Internal\", 5),\n",
    "        (\"External\", 60),\n",
    "        (\"Internal\", 32),\n",
    "        (\"External\", 4),\n",
    "        (\"Internal\", 53),\n",
    "        (\"External\", 10),\n",
    "        (\"Internal\", 37),\n",
    "        (\"External\", 41),\n",
    "        (\"Internal\", 52),\n",
    "        (\"External\", 61)\n",
    "    ]\n",
    "    columns = [\"Posting Type\", \"# Of Positions\"]\n",
    "    test_data = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    \n",
    "    # Test a categorical column with distinct count below threshold\n",
    "    is_categorical_cond, distinct_values, top_values = profile_categorical_column(test_data, \"Posting Type\")\n",
    "    assert is_categorical_cond == True\n",
    "    assert distinct_values == 2\n",
    "    assert top_values.collect() == spark.createDataFrame([('Internal',7),('External',7)],columns).collect()\n",
    "    \n",
    "    # Test a categorical column with distinct count above threshold\n",
    "    is_categorical_cond, distinct_values, top_values  = profile_categorical_column(test_data, \"# Of Positions\")\n",
    "    assert is_categorical_cond == False\n",
    "    assert distinct_values == None\n",
    "    assert top_values == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
