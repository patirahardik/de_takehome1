{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-9c6925858b4f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-9c6925858b4f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install pytest\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipytest\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/78/1891c3c5eb195ad7f41f50d38631a39d398fa9649a4fcd07d7101d7b2e06/ipytest-0.13.3-py3-none-any.whl\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.13.0)\n",
      "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (23.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (3.0.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython->ipytest) (40.8.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.16.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8; python_version < \"3.11\" in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.1.3)\n",
      "Requirement already satisfied: tomli>=1.0.0; python_version < \"3.11\" in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.6.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.1.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipytest) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.6.2)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipytest) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.1.0)\n",
      "Installing collected packages: ipytest\n",
      "Successfully installed ipytest-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding required packages\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make pyspark importable as a regular library\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pyspark related package\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profile Function Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/lib/data_profiling_transform.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Imported\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/dataproduct/assesment_nyc_job_posting/utils/spark_session.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_conf():\n",
    "    # Configure Spark settings\n",
    "    spark_conf = SparkConf()\n",
    "    spark_conf.set(\"spark.executor.instances\", \"4\") # 4 instance per node\n",
    "\n",
    "    # Set the number of executor cores\n",
    "    spark_conf.set(\"spark.executor.cores\", \"2\")  # Use 2 cores per executor\n",
    "\n",
    "    # Set the executor memory\n",
    "    spark_conf.set(\"spark.executor.memory\", \"1g\")  # Use 1GB memory per executor\n",
    "\n",
    "    # Set the driver memory\n",
    "    spark_conf.set(\"spark.driver.memory\", \"2g\")    # Use 2GB memory for the driver\n",
    "    \n",
    "    return spark_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'UnitTest'\n",
    "\n",
    "#setting spark conf before creating spark session\n",
    "spark_conf = get_spark_conf()\n",
    "    \n",
    "# Create a SparkSession with the configured settings\n",
    "spark = get_spark_session(spark_conf, job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-7.4.0, pluggy-1.2.0\n",
      "rootdir: /notebook/dataproduct/assesment_nyc_job_posting/unit_tests\n",
      "collected 1 item\n",
      "\n",
      "t_4d5bb94beeee4c44965a1487fed00126.py \u001b[31mF\u001b[0m\u001b[31m                                                      [100%]\u001b[0m\n",
      "\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_calculate_missing_value_counts ________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_calculate_missing_value_counts\u001b[39;49;00m():\n",
      "        \u001b[90m# Create a Spark DataFrame for testing\u001b[39;49;00m\n",
      "        data = [\n",
      "            (\u001b[94m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mJohn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m),\n",
      "            (\u001b[94m2\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mSmith\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "            (\u001b[94m3\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mAlice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "            (\u001b[94m4\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mBob\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mNULL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "            (\u001b[94m5\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mEve\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mDoe\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        ]\n",
      "        columns = [\u001b[33m\"\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfirst_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mlast_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        df = spark.createDataFrame(data, columns)\n",
      "    \n",
      "        \u001b[90m# Call the function to be tested\u001b[39;49;00m\n",
      "        result_df = calculate_missing_value_counts(df)\n",
      "    \n",
      "        \u001b[90m# Define the expected result DataFrame\u001b[39;49;00m\n",
      "        expected_data = [\n",
      "            (\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m),\n",
      "            (\u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m),\n",
      "            (\u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m),\n",
      "            (\u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m),\n",
      "            (\u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)\n",
      "        ]\n",
      "        expected_columns = [\u001b[33m\"\u001b[39;49;00m\u001b[33mid_missing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfirst_name_missing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mlast_name_missing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        expected_df = spark.createDataFrame(expected_data, expected_columns)\n",
      "    \n",
      "        \u001b[90m# Compare the actual and expected DataFrames\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result_df.collect() == expected_df.collect()\n",
      "\u001b[1m\u001b[31mE       assert [Row(id_missi...me_missing=3)] == [Row(id_missi...sing=0, _4=0)]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: Row(id_missing=0, first_name_missing=1, last_name_missing=3) != Row(id_missing=1, first_name_missing=0, last_name_missing=0, _4=0)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains 4 more items, first extra item: Row(id_missing=0, first_name_missing=1, last_name_missing=0, _4=0)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-15-60c8d411663c>\u001b[0m:28: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_4d5bb94beeee4c44965a1487fed00126.py::\u001b[1mtest_calculate_missing_value_counts\u001b[0m - assert [Row(id_missi...me_missing=3)] == [Row(id_missi...sing=0, _4=0)]\n",
      "\u001b[31m======================================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 3.30s\u001b[0m\u001b[31m =========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -v\n",
    "\n",
    "def test_calculate_missing_value_counts():\n",
    "    # Create a Spark DataFrame for testing\n",
    "    data = [\n",
    "        (1, \"John\", None),\n",
    "        (2, None, \"Smith\"),\n",
    "        (3, \"Alice\", \"\"),\n",
    "        (4, \"Bob\", \"NULL\"),\n",
    "        (5, \"Eve\", \"Doe\")\n",
    "    ]\n",
    "    columns = [\"id\", \"first_name\", \"last_name\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Call the function to be tested\n",
    "    result_df = calculate_missing_value_counts(df)\n",
    "\n",
    "    # Define the expected result DataFrame\n",
    "    expected_data = [\n",
    "        (1, 0, 0, 0),\n",
    "        (0, 1, 0, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 0, 0)\n",
    "    ]\n",
    "    expected_columns = [\"id_missing\", \"first_name_missing\", \"last_name_missing\"]\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_columns)\n",
    "\n",
    "    # Compare the actual and expected DataFrames\n",
    "    assert result_df.collect() == expected_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
